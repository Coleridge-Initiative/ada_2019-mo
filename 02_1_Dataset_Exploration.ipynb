{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: center;\" src=\"images/CI_horizontal.png\" width=\"600\">\n",
    "<center>\n",
    "    <span style=\"font-size: 1.5em;\">\n",
    "        <a href='https://www.coleridgeinitiative.org'>Website</a>\n",
    "    </span>\n",
    "</center>\n",
    "\n",
    "Ghani, Rayid, Frauke Kreuter, Julia Lane, Adrianne Bradford, Alex Engler, Nicolas Guetta Jeanrenaud, Graham Henke, Daniela Hochfellner, Clayton Hunter, Brian Kim, Avishek Kumar, Jonathan Morgan, Ursula Kaczmarek.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Exploration\n",
    "----------\n",
    "Basic dataset exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "## Table of Contents\n",
    "- [Introduction](#Introduction)\n",
    "    - [Learning objectives](#Learning-objectives)\n",
    "    - [Methods](#Methods)\n",
    "\n",
    "\n",
    "- [Python setup](#Python-Setup)\n",
    "\n",
    "\n",
    "- [Load the data](#Load-the-Data)\n",
    "    - [Establish a Connection to the Database](#Establish-a-Connection-to-the-Database)\n",
    "    - [Formulate Data Query](#Formulate-Data-Query)\n",
    "    - [Pull Data from the Database](#Pull-Data-from-the-Database)\n",
    "    \n",
    "\n",
    "- [Using Python and SQL](#Using-Python-and-SQL)\n",
    "    - [What is in the Database](#What-is-in-the-Database)\n",
    "   \n",
    "   \n",
    "- [Summary Statistics](#Summary-Statistics)\n",
    "    - [Query time and the PostgreSQL EXPLAIN function](#Query-time-and-the-PostgreSQL-EXPLAIN-function)\n",
    "    - [Exploring education and training data](#Exploring-education-and-training-data)\n",
    "    - [Ohio data](#Ohio-data)\n",
    "    - [Employment and Education](#Employment-and-Education)\n",
    "    \n",
    "\n",
    "- [Creating New Measures](#Creating-New-Measures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "In an ideal world, we will have all of the data we want with all of the desirable properties (no missing values, no errors, standard formats, and so on). \n",
    "However, that is hardly ever true - and we have to work with using our datasets to answer questions of interest as intelligently as possible. \n",
    "\n",
    "In this notebook, we will discover the datasets we have on the ADRF, and we will use our datasets to answer some questions of interest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objectives\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "This notebook will give you the opportunity to spend some hands-on time with the data. \n",
    "\n",
    "You will have an opportunity to explore the different datasets in the ADRF, and this notebook will take you around the different ways you can analyze your data. This involves looking at basic metrics in the larger dataset, taking a random sample, creating derived variables, making sense of the missing values, and so on. \n",
    "\n",
    "This will be done using both SQL and `pandas` (a Python package). The `sqlalchemy` Python package provides a connetion to the database to pull data into Python. \n",
    "\n",
    "This notebook will provide an introduction and examples for: \n",
    "\n",
    "- How to create new tables from the larger tables in database (sometimes called the \"analytical frame\")\n",
    "- How to explore different variables of interest\n",
    "- How to explore aggregate metrics\n",
    "- How to handle missing values\n",
    "- How to join newly created tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "We will be using the `sqlalchemy` Python package to access tables in our database server - PostgreSQL. \n",
    "\n",
    "To read the results of our queries, we will be using the `pandas` Python package, which has the ability to read tabular data from SQL queries into a pandas DataFrame object. Within `pandas`, we will use various commands:\n",
    "\n",
    "- Subsetting data\n",
    "- `groupby`\n",
    "- `merge`\n",
    "\n",
    "Within SQL, we will use various queries to:\n",
    "\n",
    "- select data subsets\n",
    "- Sum over groups\n",
    "- create new tables\n",
    "- Count distinct values of desired variables\n",
    "- Order data by chosen variables\n",
    "- Select a random sub-sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Setup\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "In Python, we `import` packages. The `import` command allows us to use libraries created by others in our own work by \"importing\" them. You can think of importing a library as opening up a toolbox and pulling out a specific tool. Among the most famous Python packages:\n",
    "- `numpy` is short for \"numerical Python\". `numpy` is a lynchpin in Python's scientific computing stack. Its strengths include a powerful *N*-dimensional array object, and a large suite of functions for doing numerical computing. \n",
    "- `pandas` is a library in Python for data analysis that uses the DataFrame object (modeled after R DataFrames, for those familiar with that language) which is similiar to a spreedsheet but allows you to do your analysis programaticaly rather than the point-and-click of Excel. It is a lynchpin of the PyData stack and is built on top of `numpy`.  \n",
    "- `sqlalchemy` is a Python library for interfacing with a PostGreSQL database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas-related imports\n",
    "import pandas as pd\n",
    "\n",
    "# database interaction imports\n",
    "import sqlalchemy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__When in doubt, use shift + tab to read the documentation of a method.__\n",
    "\n",
    "__The `help()` function provides information on what you can do with a function.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for example\n",
    "help(sqlalchemy.create_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "We can execute SQL queries using Python to get the best of both worlds. For example, Python - and pandas in particular - make it much easier to calculate descriptive statistics of the data. Additionally, as we will see in the Data Visualization exercises, it is relatively easy to create data visualizations using Python. \n",
    "\n",
    "Pandas provides many ways to load data. It allows the user to read the data from a local csv or excel file, pull the data from a relational database, or read directly from a URL (when you have internet access). Since we are working with the PostgreSQL database `appliedda` in this course, we will demonstrate how to use pandas to read data from a relational database. For examples to read data from a CSV file, refert to the pandas documentation [Getting Data In/Out](pandas.pydata.org/pandas-docs/stable/10min.html#getting-data-in-out).\n",
    "\n",
    "The function to run a SQL query and pull the data into a pandas dataframe (more to come) is `pd.read_sql()`. Just like doing a SQL query from pgAdmin, this function will ask for some information about the database, and what query you would like to run. Let's walk through the example below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establish a Connection to the Database\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "The first parameter is the connection to the database. To create a connection we will use the SQLAlchemy package and tell it which database we want to connect to, just like in pgAdmin. Additional details on creating a connection to the database are provided in the [Databases](02_1_Databases.ipynb) notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Database Connection__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# to create a connection to the database, \n",
    "# we need to pass the name of the database and host of the database\n",
    "\n",
    "host = 'stuffed.adrf.info'\n",
    "DB = 'appliedda'\n",
    "\n",
    "connection_string = \"postgresql://{}/{}\".format(host, DB)\n",
    "conn = sqlalchemy.create_engine(connection_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note we can parameterize Python `string` objects - using the built-in `.format()` function. We will use various formulations in the program notebooks (eg when building queries), some examples are:\n",
    "1. Empty brackets (shown above) which simply inserts the variable in the string; when there is more than one set of brackets Python will insert variables in the order they are listed\n",
    "2. Brackets with formatting can be used to make print statements more readable (eg `'text with formatted number with comma and 1-digit decimal {:,.1f}'.format(number_value)` will print `123,456.7` instead of `123456.7123401`)\n",
    "3. Named brackets to use the same variables multiple times in a text block (we use this in more compicated queries eg when creating \"labels\" and \"features\" for Machine Learning models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formulate Data Query\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__create a query as a `string` object in Python__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query to pull data on general studies program graduates in 2014\n",
    "query = '''\n",
    "SELECT *\n",
    "FROM mo_dhe.completions\n",
    "WHERE calyear = 2014 AND progone = '240102'\n",
    "LIMIT 20\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "- the three quotation marks surrounding the query body is called multi-line string. It is quite handy for writing SQL queries because the new line character will be considered part of the string, instead of breaking the string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have defined a variable `query`, we can call it in the code\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note that the `LIMIT` provides one simple way to get a \"sample\" of data. However, using `LIMIT` does **not provide a _random_** sample; it is just based on what is fastest for the database to return."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull Data from the Database\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Now that we have the two parameters (database connection and query), we can pass them to the `pd.read_sql()` function, and obtain the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we pass the query and the connection to the pd.read_sql() function \n",
    "df = pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first five rows of our Missouri grads pandas dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Python and SQL\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "__What are different possible measures of employment for Missouri graduates?__\n",
    "\n",
    "To explore possible metrics, we will need to combine education and employment (in our case, UI wage records) data. We will start slow and explore these datasets individually, then work up to some initial metrics of employment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is in the Database\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "As introduced in the [Databases](./02_1_Databases.ipynb) notebook, there are a few different ways to connect and explore the data in the database. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ Schemas, Tables, and Columns in database__\n",
    "\n",
    "Let's pull the list of schema names in the database, the list of tables in these schemas and the list of columns in these tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# See all available schemas:\n",
    "query = '''\n",
    "SELECT schema_name \n",
    "FROM information_schema.schemata;\n",
    "'''\n",
    "pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> As a reminder, in this class you have access to the following schemas: 'public', 'data_ohio_olda_2018', 'il_des_kcmo', 'kcmo_lehd', 'mo_dhe', 'mo_doc', 'mo_dolir', 'ada_edwork' and your team schema ('ada_edwork_#', where the # is your team number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schemas = \"\"\"\n",
    "'public', 'data_ohio_olda_2018', 'il_des_kcmo', 'kcmo_lehd', 'mo_dhe', 'mo_doc', 'mo_dolir',\n",
    "'ada_edwork', 'in_data_2019'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm our schemas exist with \n",
    "# an updated version of the previous query\n",
    "query = '''\n",
    "SELECT schema_name \n",
    "FROM information_schema.schemata\n",
    "WHERE schema_name IN ({})\n",
    "'''.format(schemas)\n",
    "pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT schemaname, tablename\n",
    "FROM pg_tables\n",
    "WHERE schemaname IN ({})\n",
    "'''.format(schemas)\n",
    "\n",
    "tables = pd.read_sql(query, conn)\n",
    "# print tables not in the public schema\n",
    "print(tables.query(\"schemaname != 'public'\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all the tables in the Missouri education schema:\n",
    "sorted(tables[tables[\"schemaname\"] == 'mo_dhe']['tablename'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note the two ways shown above to subset a `Pandas.DataFrame`:\n",
    "1. Use the built-in `.query()` function\n",
    "2. Create an array of `True` and `False` values (done in this line: `tables[\"schemaname\"] == 'mo_dhe'`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We can look at column names within tables\n",
    "# here we'll set the schema and table with variables\n",
    "\n",
    "schema = 'mo_dhe'\n",
    "tbl = 'completions'\n",
    "\n",
    "query = '''\n",
    "SELECT * \n",
    "FROM information_schema.columns \n",
    "WHERE table_schema = '{}' AND table_name = '{}'\n",
    "'''.format(schema, tbl)\n",
    "\n",
    "# read and print results\n",
    "pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Missouri education data__:\n",
    "- `mo_dhe.enrollments`: individual Missouri college/university enrolled student data\n",
    "- `mo_dhe.completions`: individual Missouri college/university graduate data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT *\n",
    "FROM mo_dhe.enrollments\n",
    "limit 100;\n",
    "'''\n",
    "df = pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT *\n",
    "FROM mo_dhe.completions\n",
    "limit 100;\n",
    "'''\n",
    "df = pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "In this section, let's start looking at aggregate statistics on the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore a few specific questions to better understand our data:\n",
    "- How are the data distributed across credential programs?\n",
    "- How do quarterly wages compare across industries?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: __ Large tables__ can take a long time to process on shared databases. The MO wage table as over 133.4M records, so we will demonstrate using SQL and Python with consideration for how much data we are reading back into Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additionally we'll use the Python time package\n",
    "# to see how long different queries takes to return\n",
    "\n",
    "import time # import time package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time() # get current time\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT count(*) \n",
    "FROM kcmo_lehd.mo_wage\n",
    "WHERE year = 2015 AND quarter = 1\n",
    "\"\"\"\n",
    "# print results\n",
    "print(pd.read_sql(query, conn)) \n",
    "# print analysis time\n",
    "print('Query returned in {:.1f} seconds'.format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example count of records for IL 2015 Q1\n",
    "\n",
    "start_time = time.time() # get current time\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT count(*) \n",
    "FROM il_des_kcmo.il_wage         \n",
    "WHERE year = 2015 AND quarter = 1\n",
    "\"\"\"\n",
    "# print results\n",
    "print(pd.read_sql(query, conn)) \n",
    "# print analysis time\n",
    "print('Query returned in {:.1f} seconds'.format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example count      \n",
    "start_time = time.time() # get current time\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT count(*) \n",
    "FROM data_ohio_olda_2018.oh_ui_wage_by_employer\n",
    "WHERE year = 2015 AND quarter = 1\n",
    "\"\"\"\n",
    "# print results\n",
    "print(pd.read_sql(query, conn))\n",
    "# print analysis time\n",
    "print('Query returned in {:.1f} seconds'.format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example count of records for 2015 Q1\n",
    "\n",
    "start_time = time.time() # get current time\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT count(*) \n",
    "FROM in_data_2019.wages_by_employer\n",
    "WHERE year = 2007 AND quarter = 2\n",
    "\"\"\"\n",
    "# print results\n",
    "print(pd.read_sql(query, conn)) \n",
    "# print analysis time\n",
    "print('Query returned in {:.1f} seconds'.format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A **question to consider**: This simple count is one measure of the total jobs in 2015 Q1. What may we want to consider when defining a \"job\" in addition to just being a row in this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the easiest way to look at a small subset of records is what\n",
    "# is already demonstrated above: simply add a LIMIT clause\n",
    "# again we'll look at how long this query takes to return\n",
    "\n",
    "# get current time\n",
    "start_time = time.time()\n",
    "\n",
    "query = '''\n",
    "SELECT *\n",
    "FROM kcmo_lehd.mo_wage\n",
    "WHERE year = 2015 AND quarter = 1\n",
    "LIMIT 20;\n",
    "'''\n",
    "# get results\n",
    "df_earnings = pd.read_sql(query, conn)\n",
    "# print analysis time\n",
    "print('Query returned in {:.1f} seconds'.format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_earnings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_earnings.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Reminder: you can refer to the documentation for more information on each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can get descriptive stats from the DataFrame:\n",
    "df_earnings.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as with the 'count` example, we will get basic stats\n",
    "# from the database using SQL; eg the wage distribution\n",
    "\n",
    "# first, just the 25th percentile value\n",
    "query = \"\"\"\n",
    "SELECT percentile_cont(0.25)\n",
    "    WITHIN GROUP (ORDER BY wage)\n",
    "FROM kcmo_lehd.mo_wage\n",
    "WHERE year = 2015 AND quarter = 1\n",
    "\"\"\"\n",
    "# display result\n",
    "pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: as of Jan 1, 2015, the minimum wage in MO was \\\\$7.65 per hour. Assuming a 35 hour work-week and 12 weeks in a quarter, someone working an entire quarter at minumum wage would earn \\\\$3,213 in the quarter (ignoring taxes).\n",
    "\n",
    "> Minimum wage in Illinois in 2015 was \\\\$10.00 per hour.\n",
    "\n",
    "> Minimum wage in Ohio in 2015 was \\\\$8.10 per hour (\\\\$4.05 for tipped employees).\n",
    "\n",
    "> As of Jan 1, 2015, minimum wage in Indiana was \\\\$7.25 per hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a list of percentile values at which to show the earnings value\n",
    "query = \"\"\"\n",
    "SELECT percentile_cont(array[0.1, 0.25, 0.5, 0.75, 0.9])\n",
    "WITHIN GROUP (ORDER BY wage)\n",
    "FROM kcmo_lehd.mo_wage\n",
    "WHERE year = 2015 AND quarter = 1\n",
    "\"\"\"\n",
    "# display result\n",
    "pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# values above return in a single cell, \n",
    "# add \"unnest\" to get values in a single cell\n",
    "# also add a reference column and column names\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT unnest(\n",
    "        percentile_cont(array[0.1, 0.25, 0.5, 0.75, 0.9])\n",
    "        WITHIN GROUP (ORDER BY wage)\n",
    "    ) AS earnings_value,\n",
    "    unnest(array[0.1, 0.25, 0.5, 0.75, 0.9]) AS percentile_value\n",
    "FROM kcmo_lehd.mo_wage\n",
    "WHERE year = 2015 AND quarter = 1\n",
    "\"\"\"\n",
    "# get the result\n",
    "df = pd.read_sql(query, conn)\n",
    "# print analysis time\n",
    "print('Query returned in {:.1f} seconds'.format(time.time()-start_time))\n",
    "# view result\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to characterize a job is the employer industry. Note the minor differences in how the wage tables are presented from the different states: for both Illinois and Missouri, we have a different table for employers and for jobs (person <-> employer combinations) and we need to get the NAICS code from the `<st>_qcew_employers` table to calculate industry earnings. In Ohio and Indiana, we do not have information about the employers beyond what is presented in the wage record data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many records from our Earnings data matches the Employers data\n",
    "# just for 2015 Q1 data\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT count(*) \n",
    "FROM kcmo_lehd.mo_wage AS earn\n",
    "JOIN kcmo_lehd.mo_qcew_employers AS empl\n",
    "ON earn.ein = empl.ein \n",
    "WHERE earn.year = 2015 AND earn.quarter = 1 \n",
    "    AND empl.year = 2015 AND empl.qtr = 1\n",
    "\"\"\"\n",
    "res = pd.read_sql(query, conn)\n",
    "print('query took {:.1f} seconds'.format(time.time()-start_time))\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view number of wage records by NAICS code for 2015 Q1\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT empl.naics, count(*) \n",
    "FROM kcmo_lehd.mo_wage AS earn\n",
    "JOIN kcmo_lehd.mo_qcew_employers AS empl\n",
    "ON earn.ein = empl.ein \n",
    "WHERE earn.year = 2015 AND earn.quarter = 1 \n",
    "    AND empl.year = 2015 AND empl.qtr = 1\n",
    "GROUP BY naics\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "res = pd.read_sql(query, conn)\n",
    "print('query took {:.1f} seconds'.format(time.time()-start_time))\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the Ohio data\n",
    "start_time = time.time()\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT naics_3_digit, count(*)\n",
    "FROM data_ohio_olda_2018.oh_ui_wage_by_employer\n",
    "WHERE year = 2015 AND quarter = 1\n",
    "GROUP BY naics_3_digit\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "# get the result\n",
    "df = pd.read_sql(query, conn)\n",
    "# print analysis time\n",
    "print('Query returned in {:.1f} seconds'.format(time.time()-start_time))\n",
    "# view result\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note that OH and IN have 3-digit codes. The industry descriptions can be looked up in the `appliedda` database table `public.naics_<year>` (lookup tables are included for `year`s 2002, 2007, 2012, 2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the Indiana data\n",
    "start_time = time.time()\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT naics_3_digit, count(*)\n",
    "FROM in_data_2019.wages_by_employer\n",
    "WHERE year = 2015 AND quarter = 1\n",
    "GROUP BY naics_3_digit\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "# get the result\n",
    "df = pd.read_sql(query, conn)\n",
    "# print analysis time\n",
    "print('Query returned in {:.1f} seconds'.format(time.time()-start_time))\n",
    "# view result\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query time and the PostgreSQL EXPLAIN function\n",
    "[Table of Contents](#Table-of-Contents)\n",
    "\n",
    "As you begin creating more advanced or complicated queries, it is useful to have a sense of how long different queries take. One way is to simple record and observe how long different queries take, as has been done in the notebook so far.\n",
    "\n",
    "You may get a sense of when a query is taking a long time. Slow queries could be the result of many people using the database at the same time, the database or table you are using are being `VACUUM`ed, or because of a poorly formed query or data structure (eg not making use of or having `indexes` on columns used to frequently subset or match the data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **The PostgreSQL \"Explain\" function** can help determine in what order the query is executed and whether it is making use of indexes. It is a little difficult to interpret, but here is an exmaple using the above Missouri query. The way to read this is start at the bottom of the RESULT and read up (steps outlined below the query output)\n",
    ">   \n",
    "    -- QUERY:\n",
    "    EXPLAIN \n",
    "    SELECT count(*) \n",
    "    FROM kcmo_lehd.mo_wage AS earn\n",
    "    JOIN kcmo_lehd.mo_qcew_employers AS empl\n",
    "    ON earn.ein = empl.ein \n",
    "    WHERE earn.year = 2015 AND earn.quarter = 1 \n",
    "        AND empl.year = 2015 AND empl.qtr = 1\n",
    ">  \n",
    "    -- RESULT\n",
    "    Aggregate  (cost=1513710.83..1513710.84 rows=1 width=0)\n",
    "      ->  Merge Join  (cost=1398108.92..1498397.93 rows=6125163 width=0)\n",
    "            Merge Cond: (empl.ein = earn.ein)\n",
    "            ->  Sort  (cost=554856.39..555314.27 rows=183152 width=65)\n",
    "                  Sort Key: empl.ein\n",
    "                  ->  Bitmap Heap Scan on mo_qcew_employers empl  (cost=90225.73..531331.95 rows=183152 width=65)\n",
    "                        Recheck Cond: ((year = 2015) AND (qtr = 1))\n",
    "                        ->  BitmapAnd  (cost=90225.73..90225.73 rows=183152 width=0)\n",
    "                              ->  Bitmap Index Scan on mo_qcew_employers_year_index  (cost=0.00..23216.37 rows=754125 width=0)\n",
    "                                    Index Cond: (year = 2015)\n",
    "                              ->  Bitmap Index Scan on mo_qcew_employers_quarter_index  (cost=0.00..66917.53 rows=2173479 width=0)\n",
    "                                    Index Cond: (qtr = 1)\n",
    "            ->  Materialize  (cost=843252.53..858244.13 rows=2998321 width=65)\n",
    "                  ->  Sort  (cost=843252.53..850748.33 rows=2998321 width=65)\n",
    "                        Sort Key: earn.ein\n",
    "                        ->  Append  (cost=0.00..397717.80 rows=2998321 width=65)\n",
    "                              ->  Seq Scan on mo_wage earn  (cost=0.00..0.00 rows=1 width=32)\n",
    "                                    Filter: ((year = 2015) AND (quarter = 1))\n",
    "                              ->  Seq Scan on mo_wage_2015q1 earn_1  (cost=0.00..397717.80 rows=2998320 width=65)\n",
    "                                    Filter: ((year = 2015) AND (quarter = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, this query runs in the following steps:\n",
    "1. scans the employer year and quarter indexes\n",
    "2. returns only rows where year and quarter meet our criteria\n",
    "3. sorts the result based on our identifier column\n",
    "4. performs same 3 steps on the earnings data (scan, filter, sort)\n",
    "5. merges the two tables based on the identifier column\n",
    "6. counts the rows (the final, \"Aggregate\" step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, try running explain on a slightly different variant of the previous query:\n",
    "\n",
    "    EXPLAIN SELECT count(*) \n",
    "    FROM il_des_kcmo.il_wage AS earn\n",
    "    JOIN il_des_kcmo.il_qcew_employers AS empl\n",
    "    ON earn.ein = empl.ein \n",
    "        AND earn.seinunit = empl.seinunit\n",
    "        AND earn.empr_no = empl.empr_no\n",
    "        AND earn.year = empl.year\n",
    "        AND earn.quarter = empl.quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's peruse a sample of Missouri data\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT earn.year, earn.quarter, earn.wage, empl.naics\n",
    "FROM kcmo_lehd.mo_wage AS earn\n",
    "JOIN kcmo_lehd.mo_qcew_employers AS empl\n",
    "ON earn.ein = empl.ein \n",
    "WHERE earn.year = 2015 AND earn.quarter = 1 \n",
    "    AND empl.year = 2015 AND empl.qtr = 1\n",
    "LIMIT 100\n",
    "\"\"\"\n",
    "res = pd.read_sql(query, conn)\n",
    "\n",
    "# report how long it took to pull data\n",
    "print('query took {:.1f} seconds'.format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the distribution of earnings in our sample?\n",
    "res['wage'].describe(percentiles=[0.1,0.25,0.5, 0.75, 0.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# and earnings by industry?\n",
    "res.groupby('naics')['wage'].describe(percentiles=[0.1,0.25,0.5, 0.75, 0.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we'll query the database for earnings distribution by 2-digit NAICS in Q1 2015\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "query=\"\"\"\n",
    "SELECT naics2, \n",
    "    unnest(percentile_cont(array[0.1,0.25,0.5, 0.75, 0.9]) \n",
    "    within group (ORDER BY wage)) AS earnings,\n",
    "    unnest(array[0.1,0.25,0.5, 0.75, 0.9]) AS percentiles\n",
    "FROM (\n",
    "    SELECT earn.year, earn.quarter, earn.wage, left(empl.naics::text, 2) naics2\n",
    "    FROM kcmo_lehd.mo_wage AS earn\n",
    "    JOIN kcmo_lehd.mo_qcew_employers AS empl\n",
    "        ON earn.ein = empl.ein \n",
    "    WHERE earn.year = 2015 AND earn.quarter = 1 \n",
    "        AND empl.year = 2015 AND empl.qtr = 1\n",
    ") subquery\n",
    "GROUP BY naics2\n",
    "ORDER BY naics2, percentiles\n",
    "\"\"\"\n",
    "res = pd.read_sql(query, conn)\n",
    "print('query took {:.1f} seconds'.format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can see from the count of non-null values in the `naics2` column if \n",
    "# there are some missing values in the data\n",
    "# if there are missing values we can view those rows with the following code\n",
    "\n",
    "res[res['naics2'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DataFrames also have useful functions like pivot tables:\n",
    "res.pivot_table(values='earnings', columns='percentiles', index='naics2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The distribution of wages paid by different industry is one way to explore how the **jobs very across industry groups.**\n",
    "\n",
    "## Exploring education and training data\n",
    "[Table of Contents](#Table-of-Contents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get a reminder of what education tables we have for Missouri\n",
    "\n",
    "query = '''\n",
    "SELECT table_name \n",
    "FROM information_schema.tables\n",
    "WHERE table_schema = 'mo_dhe'\n",
    "'''\n",
    "pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dhe is the MO Higher Ed data, see the columns\n",
    "\n",
    "query = '''\n",
    "SELECT column_name \n",
    "FROM information_schema.columns\n",
    "WHERE table_schema = 'mo_dhe'\n",
    "    AND table_name = 'completions'\n",
    "'''\n",
    "dhe_columns = pd.read_sql(query, conn)\n",
    "dhe_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check the number of records per year\n",
    "# and confirm if there is a unique SSN per year\n",
    "\n",
    "start_time = time.time()\n",
    "query = \"\"\"\n",
    "select calyear, count(*) recs, count(distinct deident_id) ind\n",
    "from mo_dhe.completions \n",
    "group by calyear\n",
    "order by calyear;\n",
    "\"\"\"\n",
    "print(pd.read_sql(query, conn))\n",
    "print('query completed in {:.2f} seconds'.format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> the \"ind\" count is slightly lower than the total \"recs\" count, indicating not each SSN is unique per year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ohio data\n",
    "[Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OH HEI data is structured in \"wide\" format, where a given individual has one row with all of their information in it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we can read all the columns into a dataframe\n",
    "query = '''\n",
    "SELECT column_name \n",
    "FROM information_schema.columns\n",
    "WHERE table_schema = 'data_ohio_olda_2018'\n",
    "    AND table_name = 'oh_hei'\n",
    "'''\n",
    "hei_columns = pd.read_sql(query, conn)\n",
    "hei_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deg_cols = [c for c in hei_columns['column_name'].values if c.startswith('deg') and c.endswith('_1')]\n",
    "len(deg_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This line of code may look complicated, so let's break it down step by step:\n",
    ">\n",
    "> 1. __`... for c in hei_columns['column_name'].values ...`__ - Loop through every element `c` in the list `hei_columns['column_name'].values`\n",
    "> 2. __`... if c.startswith('deg') and c.endswith('_1')`__ - Return only values that start with `deg` and end with `_1`\n",
    "> 3. __`c ...`__ - return value c in my new list\n",
    ">\n",
    "> _Additional Note: The formulation `[<action> for <item> in <iterable>]`is known as \"list comprehension\"._ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore 2014 HEI data\n",
    "# cannot pull 1-year into memory, need subset of columns\n",
    "start_time = time.time()\n",
    "\n",
    "sql = \"\"\"\n",
    "SELECT *\n",
    "FROM (SELECT key_id, file_year AS year,\n",
    "        unnest(array[{col_str}]) AS col_name,\n",
    "        unnest(array[{col_val}]) AS col_value\n",
    "    FROM data_ohio_olda_2018.oh_hei \n",
    "    WHERE file_year = 2014\n",
    "    ) sub_query\n",
    "WHERE col_value IS NOT NULL AND col_value <> ''\n",
    "\"\"\".format(\n",
    "col_str=','.join([\"'\"+c+\"'\" for c in deg_cols]),\n",
    "col_val=','.join([c+'::text' for c in deg_cols])\n",
    ")\n",
    "df = pd.read_sql(sql, conn)\n",
    "# print(sql)\n",
    "\n",
    "print('query completed in {:.2f} seconds'.format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many people earned at least one degree/certificate in 2014?\n",
    "df['key_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['col_name'].str.contains('subject')].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Subject codes can be looked up in the `data_ohio_olda_2018.oh_cip_to_soc_crosswalk` table; note the CIP codes are formatted `##.####`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many subjects were studied?\n",
    "df[df['col_name'].str.contains('subject')]['col_value'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Employment and Education\n",
    "[Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Now we'll explore how many OH 2014 grads were **employed** in Ohio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "sql = \"\"\"\n",
    "CREATE TEMP TABLE oh_hei_2014 AS\n",
    "SELECT * \n",
    "FROM (SELECT key_id, file_year AS year,\n",
    "        unnest(array[{col_str}]) AS col_name,\n",
    "        unnest(array[{col_val}]) AS col_value\n",
    "    FROM data_ohio_olda_2018.oh_hei \n",
    "    WHERE file_year = 2014) sub_query\n",
    "WHERE col_value IS NOT NULL AND col_value <> ''\n",
    "\"\"\".format(\n",
    "col_str=','.join([\"'\"+c+\"'\" for c in deg_cols]),\n",
    "col_val=','.join([c+'::text' for c in deg_cols])\n",
    ")\n",
    "# print(sql)\n",
    "conn.execute(sql)\n",
    "\n",
    "print('query completed in {:.2f} seconds'.format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count records and distinct people in temp table to confirm it's the same as we expect\n",
    "pd.read_sql('SELECT count(*) recs, count(distinct key_id) people from oh_hei_2014', conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many of the 2014 graduates were employed in 2015?\n",
    "start_time = time.time()\n",
    "\n",
    "sql = \"\"\"\n",
    "select count(*) job_qtrs, count(distinct key_id) employees\n",
    "from data_ohio_olda_2018.oh_ui_wage_by_quarter\n",
    "where year = 2015\n",
    "and key_id IN (select distinct key_id from oh_hei_2014 )\n",
    "\"\"\"\n",
    "print(pd.read_sql(sql, conn))\n",
    "print('query completed in {:.2f} seconds'.format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating New Measures\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "> **Questions to consider** (we will revisit similar questions of measurement frequently during the program)\n",
    "0. What problem are we working to solve? How can we measure it?\n",
    "1. How should we define that an individual \"received a job\"? For example, definitions could be \n",
    "  * Received greater than 0 pay at some point within 1 year\n",
    "  * Received greater than minimum wage (assuming XyZ) in 6 of 8 quarters after graduating\n",
    "2. How narrowly can you define the unit of analysis? Eg an individual who graduated in year Y...\n",
    "3. What additional information do we know about these individuals?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preliminary Examples**\n",
    "\n",
    "As the notebooks progress we will dig into different aspects of the above questions, but for now we will show an example of defining MO graduates' employment outcomes: employed in MO for the first quarter of the year after graduation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 2014 MO grads dataframe\n",
    "sql = \"\"\"\n",
    "SELECT *\n",
    "FROM mo_dhe.completions\n",
    "WHERE calyear = 2014;\n",
    "\"\"\"\n",
    "\n",
    "df_grads = pd.read_sql(sql, conn)\n",
    "df_grads.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read 2015 Q1 jobs into dataframe\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "sql = \"\"\"\n",
    "SELECT *\n",
    "FROM kcmo_lehd.mo_wage\n",
    "WHERE year = 2015 \n",
    "    AND quarter = 1\n",
    "\"\"\"\n",
    "\n",
    "df_jobs = pd.read_sql(sql, conn)\n",
    "\n",
    "print('query completed in {:.2f} seconds'.format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of unique individuals in jobs dataframe\n",
    "df_jobs['ssn'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine grads and jobs dataframe to grads dataframe on ID/SSN columns\n",
    "df = pd.merge(df_grads, df_jobs, left_on = 'deident_id', right_on = 'ssn')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{:,.0f} of {:,.0f} 2014 graduates are present in MO 2015 wage data'.format(df['ssn'].nunique(), \n",
    "                                                                             df_grads['deident_id'].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# what are the distribution of earnings?\n",
    "df['wage'].describe(percentiles=[.1, .25, .5, .75, .9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many graduates made more than $3,000 in each of 2 or more quarters?\n",
    "sum(df_jobs[df_jobs['wage']>3000].groupby('ssn')['id'].count() > 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Separate example: Replicating the QWI Statistics__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The QWI Statistics notebook demonstrates another example of feature creation: the Quarterly Workforce Indicators Census framework using IL wage records."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3-ada",
   "language": "python",
   "name": "py3-ada"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {
    "height": "566px",
    "left": "0px",
    "right": "954px",
    "top": "110px",
    "width": "179px"
   },
   "toc_section_display": "none",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
